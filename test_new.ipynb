{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.imagenet1k import make_imagenet1k\n",
    "import yaml\n",
    "import torch\n",
    "import pprint\n",
    "from src.transforms import make_transforms\n",
    "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
    "from src.utils.distributed import (\n",
    "    init_distributed,\n",
    "    AllReduce\n",
    ")\n",
    "\n",
    "from src.helper import (\n",
    "    load_checkpoint,\n",
    "    init_model,\n",
    "    init_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './configs/in100_vitt_ep1.yaml'\n",
    "fname = './configs/dev.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'data': {   'batch_size': 64,\n",
      "                'color_jitter_strength': 0.0,\n",
      "                'crop_scale': [0.3, 1.0],\n",
      "                'crop_size': 224,\n",
      "                'image_folder': 'data/imagenet_100/',\n",
      "                'num_workers': 10,\n",
      "                'pin_mem': True,\n",
      "                'root_path': '/localscratch/hsun409/',\n",
      "                'use_color_distortion': False,\n",
      "                'use_gaussian_blur': False,\n",
      "                'use_horizontal_flip': False},\n",
      "    'logging': {   'folder': '/localscratch/hsun409/logs/ijepa/test/',\n",
      "                   'write_tag': 'jepa'},\n",
      "    'mask': {   'allow_overlap': False,\n",
      "                'aspect_ratio': [0.75, 1.5],\n",
      "                'enc_mask_scale': [0.85, 1.0],\n",
      "                'min_keep': 10,\n",
      "                'num_enc_masks': 1,\n",
      "                'num_pred_masks': 4,\n",
      "                'patch_size': 14,\n",
      "                'pred_mask_scale': [0.15, 0.2]},\n",
      "    'meta': {   'copy_data': False,\n",
      "                'load_checkpoint': False,\n",
      "                'model_name': 'vit_tiny',\n",
      "                'pred_depth': 12,\n",
      "                'pred_emb_dim': 384,\n",
      "                'read_checkpoint': None,\n",
      "                'use_bfloat16': False},\n",
      "    'optimization': {   'ema': [0.996, 1.0],\n",
      "                        'epochs': 1,\n",
      "                        'final_lr': 1e-06,\n",
      "                        'final_weight_decay': 0.4,\n",
      "                        'ipe_scale': 1.0,\n",
      "                        'lr': 0.001,\n",
      "                        'start_lr': 0.0002,\n",
      "                        'warmup': 40,\n",
      "                        'weight_decay': 0.04}}\n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as y_file:\n",
    "    args = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint']\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "\n",
    "\n",
    "# -- DATA\n",
    "use_gaussian_blur = args['data']['use_gaussian_blur']\n",
    "use_horizontal_flip = args['data']['use_horizontal_flip']\n",
    "use_color_distortion = args['data']['use_color_distortion']\n",
    "color_jitter = args['data']['color_jitter_strength']\n",
    "# --\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "image_folder = args['data']['image_folder']\n",
    "crop_size = args['data']['crop_size']\n",
    "crop_scale = args['data']['crop_scale']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not torch.cuda.is_available():\n",
    "#     device = torch.device('cpu')\n",
    "# else:\n",
    "#     device = torch.device('cuda:0')\n",
    "#     torch.cuda.set_device(device)\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:making imagenet data transforms\n",
      "INFO:root:SLURM vars not set (distributed training not available)\n",
      "INFO:root:data-path /localscratch/hsun409/data/imagenet_100/train/\n",
      "INFO:root:Initialized ImageNet\n",
      "INFO:root:ImageNet dataset created\n",
      "INFO:root:ImageNet unsupervised data loader created\n"
     ]
    }
   ],
   "source": [
    "transform = make_transforms(\n",
    "    crop_size=crop_size,\n",
    "    crop_scale=crop_scale,\n",
    "    gaussian_blur=use_gaussian_blur,\n",
    "    horizontal_flip=use_horizontal_flip,\n",
    "    color_distortion=use_color_distortion,\n",
    "    color_jitter=color_jitter)\n",
    "\n",
    "\n",
    "mask_collator = MBMaskCollator(\n",
    "    input_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    pred_mask_scale=pred_mask_scale,\n",
    "    enc_mask_scale=enc_mask_scale,\n",
    "    aspect_ratio=aspect_ratio,\n",
    "    nenc=num_enc_masks,\n",
    "    npred=num_pred_masks,\n",
    "    allow_overlap=allow_overlap,\n",
    "    min_keep=min_keep)\n",
    "\n",
    "world_size, rank = init_distributed()\n",
    "\n",
    "_, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
    "        transform=transform,\n",
    "        batch_size=batch_size,\n",
    "        collator=mask_collator,\n",
    "        pin_mem=pin_mem,\n",
    "        training=True,\n",
    "        num_workers=num_workers,\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "        root_path=root_path,\n",
    "        image_folder=image_folder,\n",
    "        copy_data=copy_data,\n",
    "        drop_last=True)\n",
    "\n",
    "for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
    "\n",
    "    def load_imgs():\n",
    "        # -- unsupervised imgs\n",
    "        imgs = udata[0].to(device, non_blocking=True)\n",
    "        masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "        masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "        return (imgs, masks_1, masks_2)\n",
    "    imgs, masks_enc, masks_pred = load_imgs()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udata[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udata[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "\n",
    "        return x, pos_bool, mask_no_pos\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, mask_no_pos = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'The function is only tested when pos are dropped.'\n",
    "            logits = self.forward_decoder(x)\n",
    "            return x, logits, pos_bool, mask_no_pos\n",
    "\n",
    "        else:\n",
    "            return x  # The classical IJEPA\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "        pos_labels = torch.sort(mask_no_pos.detach(), dim=1).values\n",
    "\n",
    "        return x, pos_bool, pos_labels\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, pos_labels = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'The function is only tested when pos are dropped.'\n",
    "            logits = self.forward_decoder(x)\n",
    "            return x, logits, pos_bool, pos_labels\n",
    "\n",
    "        else:\n",
    "            return x  # The classical IJEPA\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "        x_initial = x.clone()\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        # x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        # x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "        pos_labels = torch.sort(mask_no_pos.detach(), dim=1).values\n",
    "\n",
    "        return x, pos_bool, pos_labels, x_initial, mask_no_pos\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            print('here')\n",
    "            x, pos_bool, pos_labels, x_initial, mask_no_pos = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "            return x, pos_bool, pos_labels, x_initial, mask_no_pos\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'The function is only tested when pos are dropped.'\n",
    "            logits = self.forward_decoder(x)\n",
    "            return x, logits, pos_bool, pos_labels\n",
    "\n",
    "        else:\n",
    "            return x  # The classical IJEPA\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = encoder.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, logits, pos_bool, labels = result\n",
    "\n",
    "logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)\n",
    "\n",
    "loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, pos_bool, labels, x_initial, mask_no_pos = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)\n",
    "\n",
    "loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6027, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, pos_bool, labels, x_initial, mask_no_pos = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,  24, 140,  ...,  57, 109,  26],\n",
       "        [144, 123,  93,  ..., 224,  45, 163],\n",
       "        [ 58,   4, 139,  ..., 108,  25,  73],\n",
       "        ...,\n",
       "        [ 58,  13, 108,  ..., 142, 138, 109],\n",
       "        [ 34, 115, 112,  ...,  94,  64,  97],\n",
       "        [130,  81, 131,  ...,  30, 100,  98]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tensor, sorted_indices = torch.sort(mask_no_pos, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 156, 157, 158],\n",
       "        [  3,   5,  12,  ..., 225, 226, 238],\n",
       "        [  3,   4,   8,  ..., 140, 142, 154],\n",
       "        ...,\n",
       "        [  1,  13,  14,  ..., 138, 139, 142],\n",
       "        [  3,   7,  14,  ..., 129, 142, 144],\n",
       "        [  0,  12,  18,  ..., 130, 131, 144]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_drop_pos_from_initial = apply_masks(x_initial, [sorted_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_drop_pos_from_function = x[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, x_initial.size(2))].reshape(\n",
    "        batch_size, -1, x_initial.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_drop_pos_from_function != x_drop_pos_from_initial).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches**.5),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "\n",
    "        # Notice that the labels are sorted as the original order\n",
    "        pos_labels = torch.sort(mask_no_pos.detach(), dim=1).values\n",
    "\n",
    "        return x, pos_bool, pos_labels\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "\n",
    "            pos_bool, pos_labels = None, None\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, pos_labels = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'The function is only tested when pos are dropped.'\n",
    "            logits = self.forward_decoder(x)\n",
    "            return x, logits, pos_bool, pos_labels\n",
    "\n",
    "        else:\n",
    "            return x  # The classical IJEPA\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on the edits:\n",
    "    - Add a lightweight decoder with linear head for pos prediction\n",
    "    - Add a careful pos dropping strategy based on current masking approach\n",
    "\"\"\"\n",
    "\n",
    "from src.masks.utils import apply_masks\n",
    "from functools import partial\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches**.5),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        # This function will be used in the forward part\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Preparation\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Drop the positions\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Case 1: Replace pos_embed with mask_pos_token\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        # Case 2: Retain the pos_embed\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the variables that we want\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "\n",
    "        # Notice that the labels are sorted as the original order\n",
    "        pos_labels = torch.sort(mask_no_pos.detach(), dim=1).values\n",
    "\n",
    "        # x.shape = (B, N_m, D)\n",
    "        # pos_bool.shape = (B, N_m)\n",
    "        # pos_labels.shape = (B, int(N_m * pos_drop_ratio))\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        return x, pos_bool, pos_labels\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        \"\"\"\n",
    "        masks: a list of masks; for context there should only be one mask.\n",
    "            each mask has shape (batch_size, no. patches to keep)\n",
    "\n",
    "        pos_drop_ratio: the ratio to drop the positions from the context patches\n",
    "\n",
    "        user_decoder: we apply a lightweight decoder for position prediction.\n",
    "            if we use decoder, we return additional pos_logits, pos_bool, pos_labels\n",
    "        \"\"\"\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Handle the mask\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the positional embeddings for each patch\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "            pos_bool, pos_labels = None, None\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # When we drop the positional embeddings:\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, pos_labels = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Forward and apply norm\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the position prediction results\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'Only tested when pos are dropped.'\n",
    "            pos_logits = self.forward_decoder(x)\n",
    "            return x, pos_logits, pos_bool, pos_labels\n",
    "\n",
    "            # Usage for the logits\n",
    "            # logits.shape = [B, N_m, N_patches]\n",
    "            # pos_labels.shape = [B, int(N_m * pos_drop_ratio)]\n",
    "            # We don't predict the labels for those with pos_emb\n",
    "            # so we should do:\n",
    "            # logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "            #          -1, -1, N_patches)].reshape(\n",
    "            #          batch_size, -1, N_patches)\n",
    "            # Here N_patches essentially is N_classes for positions\n",
    "            # loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)\n",
    "\n",
    "        # If not use decoder, just classical IJEPA\n",
    "        else:\n",
    "            return x\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on the edits:\n",
    "    - Add a lightweight decoder with linear head for pos prediction\n",
    "    - Add a careful pos dropping strategy based on current masking approach\n",
    "\"\"\"\n",
    "\n",
    "from src.masks.utils import apply_masks\n",
    "from typing import List, Tuple\n",
    "from functools import partial\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "                                                      int(num_patches**.5),\n",
    "                                                      cls_token=False)\n",
    "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bs, seq_len, dims = x.shape\n",
    "        x = x.permute((0, 2, 1))\n",
    "        return self.avg_pool(x).squeeze()\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.avg_pool = FeatAvgPool()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        \"\"\"\n",
    "        Helper functions to be used in the forward part to drop positions.\n",
    "        \"\"\"\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Preparation\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Drop the positions\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Case 1: Replace pos_embed with mask_pos_token\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        # Case 2: Retain the pos_embed\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the variables that we want\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "\n",
    "        # Notice that the labels are sorted as the original order\n",
    "        pos_labels = torch.sort(mask_no_pos.detach(), dim=1).values\n",
    "\n",
    "        # x.shape = (B, N_m, D)\n",
    "        # pos_bool.shape = (B, N_m)\n",
    "        # pos_labels.shape = (B, int(N_m * pos_drop_ratio))\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        return x, pos_bool, pos_labels\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "        \"\"\"\n",
    "        This will be used at the forward part to\n",
    "        get the logits for positions.\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x,\n",
    "                masks=None,\n",
    "                pos_drop_ratio: float=0,\n",
    "                use_pos_predictor: bool=False,\n",
    "                out_feat_keys: List[str]=None):\n",
    "        \"\"\"\n",
    "        masks: a list of masks; for context there should only be one mask.\n",
    "            each mask has shape (batch_size, no. patches to keep)\n",
    "\n",
    "        pos_drop_ratio: the ratio to drop the positions from the context patches\n",
    "\n",
    "        user_decoder: we apply a lightweight decoder for position prediction.\n",
    "            if we use decoder, we return additional pos_logits, pos_bool, pos_labels\n",
    "        \"\"\"\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get features for the evaluation; see methods at the end\n",
    "        if out_feat_keys:\n",
    "            x = self.get_intermediate_features(x, masks, out_feat_keys)\n",
    "            return x\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Handle the mask\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the positional embeddings for each patch\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "            pos_bool, pos_labels = None, None\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # When we drop the positional embeddings:\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, pos_labels = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Forward and apply norm\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the position prediction results\n",
    "        if use_pos_predictor:\n",
    "            assert pos_drop_ratio, 'Only tested when pos are dropped.'\n",
    "            pos_logits = self.forward_decoder(x)\n",
    "            return x, pos_logits, pos_bool, pos_labels\n",
    "\n",
    "            # Usage for the logits\n",
    "            # logits.shape = [B, N_m, N_patches]\n",
    "            # pos_labels.shape = [B, int(N_m * pos_drop_ratio)]\n",
    "            # We don't predict the labels for those with pos_emb\n",
    "            # so we should do:\n",
    "            # logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "            #          -1, -1, N_patches)].reshape(\n",
    "            #          batch_size, -1, N_patches)\n",
    "            # Here N_patches essentially is N_classes for positions\n",
    "            # loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)\n",
    "\n",
    "        # If not use decoder, just classical IJEPA\n",
    "        else:\n",
    "            return x\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "    # ======================================================================= #\n",
    "    # Helper functions to get features (will be called with no_grad for eval)\n",
    "    def prepare_tokens(self, x: torch.Tensor, masks=None) -> torch.Tensor:\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # We shouldn't use mask for eval, but let's just keep it here\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------- #\n",
    "        # Get the positional embeddings for each patch\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # In evaluation we will always add the pos_emb\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # We shouldn't use mask for eval, but let's just keep it here\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_intermediate_features(self, x, masks=None,\n",
    "            names: List[str]=None) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Given a list of feature names, return a list of the same length\n",
    "        where each output correspond to the desired feature.\n",
    "\n",
    "        To align with ijepa, the available features are:\n",
    "        - lastpool\n",
    "        - concatpool4\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare tokens (patchify and add positional encoding)\n",
    "        x = self.prepare_tokens(x, masks)\n",
    "\n",
    "        # Determine the number of layers to keep based on requested features\n",
    "        keep_last_n = 1 if 'lastpool' in names else 0\n",
    "        if any(name.startswith('concatpool') for name in names):\n",
    "            keep_last_n = max(keep_last_n, 4)  # Keep last 4 for concatPOOL4\n",
    "\n",
    "        # Buffer to store outputs of the required last N layers\n",
    "        interms_buffer = collections.deque(maxlen=keep_last_n)\n",
    "\n",
    "        # Forward propagation\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "            # Append to buffer if in the last N layers\n",
    "            if i >= len(self.blocks) - keep_last_n:\n",
    "                interms_buffer.append(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "            interms_buffer[-1] = x\n",
    "\n",
    "        output = []\n",
    "        for name in names:\n",
    "            if name == 'lastpool':\n",
    "                output.append(self.avg_pool(interms_buffer[-1]))\n",
    "            elif name.startswith('concatpool'):\n",
    "                concat_features = torch.cat([self.avg_pool(layer) for layer in interms_buffer], dim=-1)\n",
    "                output.append(concat_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (avg_pool): FeatAvgPool(\n",
       "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(14, 14), stride=(14, 14))\n",
       "  )\n",
       "  (decoder_embed): Linear(in_features=192, out_features=256, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-1): 2 x Block(\n",
       "      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    decoder_embed_dim=256,\n",
    "    decoder_num_heads=2,\n",
    "    decoder_depth=2).to(device)\n",
    "\n",
    "num_patches = encoder.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6027, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_pos_predictor=True)\n",
    "\n",
    "x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)\n",
    "\n",
    "pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "pos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6027, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_decoder=True)\n",
    "\n",
    "x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)\n",
    "\n",
    "pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "pos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0316, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_pos_predictor=False)\n",
    "\n",
    "# x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "# pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "#     -1, -1, num_patches)].reshape(\n",
    "#         batch_size, -1, num_patches)\n",
    "\n",
    "# pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "print(result[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0316, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_decoder=False)\n",
    "\n",
    "# x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "# pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "#     -1, -1, num_patches)].reshape(\n",
    "#         batch_size, -1, num_patches)\n",
    "\n",
    "# pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "print(result[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.3050, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0, use_decoder=False)\n",
    "\n",
    "# x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "# pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "#     -1, -1, num_patches)].reshape(\n",
    "#         batch_size, -1, num_patches)\n",
    "\n",
    "# pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "print(result[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.3018, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = encoder(imgs, None, pos_drop_ratio=0, use_decoder=False)\n",
    "\n",
    "# x, pos_logits, pos_bool, pos_labels = result\n",
    "\n",
    "# pos_logits = pos_logits[pos_bool.unsqueeze(-1).expand(\n",
    "#     -1, -1, num_patches)].reshape(\n",
    "#         batch_size, -1, num_patches)\n",
    "\n",
    "# pos_loss = F.cross_entropy(pos_logits.permute(0, 2, 1), pos_labels)\n",
    "print(result[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 73, 256])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29, 256])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, logits, pos_bool, labels = result\n",
    "\n",
    "logits = logits[pos_bool.unsqueeze(-1).expand(\n",
    "    -1, -1, num_patches)].reshape(\n",
    "        batch_size, -1, num_patches)\n",
    "\n",
    "loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6027, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 73])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
