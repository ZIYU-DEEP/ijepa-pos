{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.imagenet1k import make_imagenet1k\n",
    "import yaml\n",
    "import torch\n",
    "import pprint\n",
    "from src.transforms import make_transforms\n",
    "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
    "from src.utils.distributed import (\n",
    "    init_distributed,\n",
    "    AllReduce\n",
    ")\n",
    "\n",
    "from src.helper import (\n",
    "    load_checkpoint,\n",
    "    init_model,\n",
    "    init_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './configs/in100_vitt_ep1.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'data': {   'batch_size': 64,\n",
      "                'color_jitter_strength': 0.0,\n",
      "                'crop_scale': [0.3, 1.0],\n",
      "                'crop_size': 224,\n",
      "                'image_folder': 'data/imagenet_100/',\n",
      "                'num_workers': 10,\n",
      "                'pin_mem': True,\n",
      "                'root_path': '/localscratch/hsun409/',\n",
      "                'use_color_distortion': False,\n",
      "                'use_gaussian_blur': False,\n",
      "                'use_horizontal_flip': False},\n",
      "    'logging': {   'folder': '/localscratch/hsun409/logs/ijepa/test/',\n",
      "                   'write_tag': 'jepa'},\n",
      "    'mask': {   'allow_overlap': False,\n",
      "                'aspect_ratio': [0.75, 1.5],\n",
      "                'enc_mask_scale': [0.85, 1.0],\n",
      "                'min_keep': 10,\n",
      "                'num_enc_masks': 1,\n",
      "                'num_pred_masks': 4,\n",
      "                'patch_size': 14,\n",
      "                'pred_mask_scale': [0.15, 0.2]},\n",
      "    'meta': {   'copy_data': False,\n",
      "                'load_checkpoint': False,\n",
      "                'model_name': 'vit_tiny',\n",
      "                'pred_depth': 12,\n",
      "                'pred_emb_dim': 384,\n",
      "                'read_checkpoint': None,\n",
      "                'use_bfloat16': False},\n",
      "    'optimization': {   'ema': [0.996, 1.0],\n",
      "                        'epochs': 1,\n",
      "                        'final_lr': 1e-06,\n",
      "                        'final_weight_decay': 0.4,\n",
      "                        'ipe_scale': 1.0,\n",
      "                        'lr': 0.001,\n",
      "                        'start_lr': 0.0002,\n",
      "                        'warmup': 40,\n",
      "                        'weight_decay': 0.04}}\n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as y_file:\n",
    "    args = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bfloat16 = args['meta']['use_bfloat16']\n",
    "model_name = args['meta']['model_name']\n",
    "load_model = args['meta']['load_checkpoint']\n",
    "r_file = args['meta']['read_checkpoint']\n",
    "copy_data = args['meta']['copy_data']\n",
    "pred_depth = args['meta']['pred_depth']\n",
    "pred_emb_dim = args['meta']['pred_emb_dim']\n",
    "\n",
    "\n",
    "# -- DATA\n",
    "use_gaussian_blur = args['data']['use_gaussian_blur']\n",
    "use_horizontal_flip = args['data']['use_horizontal_flip']\n",
    "use_color_distortion = args['data']['use_color_distortion']\n",
    "color_jitter = args['data']['color_jitter_strength']\n",
    "# --\n",
    "batch_size = args['data']['batch_size']\n",
    "pin_mem = args['data']['pin_mem']\n",
    "num_workers = args['data']['num_workers']\n",
    "root_path = args['data']['root_path']\n",
    "image_folder = args['data']['image_folder']\n",
    "crop_size = args['data']['crop_size']\n",
    "crop_scale = args['data']['crop_scale']\n",
    "# --\n",
    "\n",
    "# -- MASK\n",
    "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
    "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
    "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
    "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
    "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
    "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
    "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
    "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
    "# --\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "ema = args['optimization']['ema']\n",
    "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
    "wd = float(args['optimization']['weight_decay'])\n",
    "final_wd = float(args['optimization']['final_weight_decay'])\n",
    "num_epochs = args['optimization']['epochs']\n",
    "warmup = args['optimization']['warmup']\n",
    "start_lr = args['optimization']['start_lr']\n",
    "lr = args['optimization']['lr']\n",
    "final_lr = args['optimization']['final_lr']\n",
    "\n",
    "# -- LOGGING\n",
    "folder = args['logging']['folder']\n",
    "tag = args['logging']['write_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:making imagenet data transforms\n"
     ]
    }
   ],
   "source": [
    "transform = make_transforms(\n",
    "    crop_size=crop_size,\n",
    "    crop_scale=crop_scale,\n",
    "    gaussian_blur=use_gaussian_blur,\n",
    "    horizontal_flip=use_horizontal_flip,\n",
    "    color_distortion=use_color_distortion,\n",
    "    color_jitter=color_jitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.3, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = MBMaskCollator(\n",
    "    input_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    pred_mask_scale=pred_mask_scale,\n",
    "    enc_mask_scale=enc_mask_scale,\n",
    "    aspect_ratio=aspect_ratio,\n",
    "    nenc=num_enc_masks,\n",
    "    npred=num_pred_masks,\n",
    "    allow_overlap=allow_overlap,\n",
    "    min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:SLURM vars not set (distributed training not available)\n"
     ]
    }
   ],
   "source": [
    "world_size, rank = init_distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/imagenet_100/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:data-path /localscratch/hsun409/data/imagenet_100/train/\n",
      "INFO:root:Initialized ImageNet\n",
      "INFO:root:ImageNet dataset created\n",
      "INFO:root:ImageNet unsupervised data loader created\n"
     ]
    }
   ],
   "source": [
    "_, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
    "        transform=transform,\n",
    "        batch_size=batch_size,\n",
    "        collator=mask_collator,\n",
    "        pin_mem=pin_mem,\n",
    "        training=True,\n",
    "        num_workers=num_workers,\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "        root_path=root_path,\n",
    "        image_folder=image_folder,\n",
    "        copy_data=copy_data,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
    "\n",
    "    def load_imgs():\n",
    "        # -- unsupervised imgs\n",
    "        imgs = udata[0].to(device, non_blocking=True)\n",
    "        masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "        masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "        return (imgs, masks_1, masks_2)\n",
    "    imgs, masks_enc, masks_pred = load_imgs()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks = masks_enc \n",
    "# mask = masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "                \n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "        x_initial = x.clone()  # Save the original x for later use\n",
    "\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        # x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        # x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x_restored = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # # Create a mask to identify positions with dropped positional embeddings\n",
    "        # pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)  # Adjust mask size to [B, N_m]\n",
    "        # drop_indices_restored = restored_indices[:, :num_pos_to_drop]\n",
    "\n",
    "        # # Use advanced indexing to set the dropped positions to True\n",
    "        # batch_indices = torch.arange(B, device=device).view(-1, 1)\n",
    "        # pos_drop_mask[batch_indices, drop_indices_restored] = True\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_drop_mask = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "\n",
    "        return x_no_pos, x_restored, pos_drop_mask, x_initial, mask_no_pos\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "            if masks is not None: x = apply_masks(x, masks)\n",
    "            mask_test = None, None\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x_no_pos, x, pos_drop_mask, x_initial, mask_no_pos = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "            \n",
    "            return x_no_pos, x, pos_drop_mask, x_initial, mask_no_pos\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            x = self.forward_decoder(x)\n",
    "            x = self.decoder_pred(x)  # from embed_dim to num_patches\n",
    "            return x, pos_drop_mask \n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5080,  0.7931,  0.0418, -0.9466],\n",
      "        [ 0.4480,  1.2964, -1.4254, -1.4578],\n",
      "        [ 0.7382, -0.0849, -0.0025, -0.9821],\n",
      "        [ 1.0544,  1.7277, -0.5480,  1.0907]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2, 0, 1],\n",
       "        [3, 2, 0, 1],\n",
       "        [3, 1, 2, 0],\n",
       "        [2, 0, 3, 1]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "\n",
    "\n",
    "torch.argsort(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 192])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]], device='cuda:0')"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim).to(device)\n",
    "\n",
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.2)\n",
    "x_no_pos, x, pos_drop_mask, x_initial, mask_no_pos = result\n",
    "kk = x[pos_drop_mask.unsqueeze(-1).expand(-1, -1, 192)].view(batch_size, -1, 192)\n",
    "dd = apply_masks(x_initial, [mask_no_pos])\n",
    "sorted_tensor, sorted_indices = torch.sort(mask_no_pos, dim=1)\n",
    "ee = apply_masks(x_initial, [sorted_tensor])\n",
    "kk == ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2089e2c110>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_no_pos, x, pos_drop_mask, x_initial, mask_no_pos = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = x[pos_drop_mask.unsqueeze(-1).expand(-1, -1, 192)].view(batch_size, -1, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = apply_masks(x_initial, [mask_no_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21,   1, 110,  ..., 109,   4,  75],\n",
       "        [ 66,  16,  80,  ...,  32,  46,  81],\n",
       "        [ 44,  67, 130,  ...,  34,  84, 179],\n",
       "        ...,\n",
       "        [192,  45, 198,  ...,  59,   6, 201],\n",
       "        [ 27,  14,   7,  ...,  48,  28,  67],\n",
       "        [141,  60, 200,  ..., 204, 196, 110]], device='cuda:0')"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tensor, sorted_indices = torch.sort(mask_no_pos, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   3,   4,  ..., 109, 110, 178],\n",
       "        [  6,  16,  19,  ...,  92,  97, 109],\n",
       "        [ 20,  32,  33,  ..., 160, 179, 192],\n",
       "        ...,\n",
       "        [  6,   9,  12,  ..., 193, 198, 201],\n",
       "        [  3,   6,   7,  ..., 112, 119, 158],\n",
       "        [ 26,  29,  42,  ..., 196, 200, 204]], device='cuda:0')"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = apply_masks(x_initial, [sorted_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(178, device='cuda:0')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask_no_pos[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5, device='cuda:0')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask_no_pos[0].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 21,   1, 110,  13,   3, 178,  32,  27,  40,  16,  49,  51,  24, 109,\n",
       "          4,  75], device='cuda:0')"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4, 14,  3,  9,  0, 12,  7,  6,  8, 10, 11, 15, 13,  2,  5],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_no_pos[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]], device='cuda:0')"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk == ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1167, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk[0, 4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5047, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[0, -2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1167, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_initial[0, 16, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 192])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 192])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_no_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 192])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_drop_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = x[pos_drop_mask.unsqueeze(-1).expand(-1, -1, 192)].view(batch_size, -1, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 192])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3067,  0.4483, -1.2291,  0.8508, -0.2626, -2.4275, -1.2877,  0.6114,\n",
       "        -0.4339, -0.5747,  0.1223, -0.1771,  0.8479, -1.1140,  0.1051,  1.4932,\n",
       "         2.1093, -0.1131,  1.2370, -0.1574,  0.3884, -1.4170,  0.1403,  0.3876,\n",
       "        -0.0950,  0.8172, -0.4414,  1.6031, -0.8563,  0.2816,  0.4774, -0.1175,\n",
       "        -1.3723, -0.8352,  1.1400,  0.7687, -0.6594, -0.2641,  1.5124, -0.8623,\n",
       "        -0.9539,  1.1319, -1.2194, -0.4562, -0.1077, -1.0779, -0.3600, -1.2942,\n",
       "         1.5568,  0.7144, -0.9447, -0.4813, -0.0278, -0.4884, -0.9560, -0.2270,\n",
       "        -0.6050, -0.3337, -0.8963,  0.2019, -1.4995, -1.3187, -1.0790, -0.7769,\n",
       "        -0.7151, -0.6373, -0.5240, -0.0088, -0.3877,  0.0799,  0.9536,  0.4070,\n",
       "        -0.4604, -0.8042, -0.0260,  0.8930, -2.5984, -0.9909, -0.3840,  0.5446,\n",
       "        -0.8373, -0.9194,  0.9641, -0.9640,  0.8740,  0.4703,  0.8944,  4.0453,\n",
       "         0.8769, -1.2336, -0.6158, -0.0587, -0.0862,  0.2332, -0.2544, -1.8226,\n",
       "        -0.0378,  0.1936, -0.2143,  0.4674, -0.0550, -0.9369, -0.2416, -0.8570,\n",
       "        -0.4199, -1.7930, -0.5012, -1.2116, -0.0473,  0.9123, -1.5088,  0.3307,\n",
       "        -0.0490, -0.2292,  2.0112, -1.1865, -0.7555, -0.8932, -0.5968, -1.1088,\n",
       "         0.1603, -0.3854,  2.1423,  0.5650, -0.4524,  0.3934, -1.8120,  0.8335,\n",
       "         1.3183,  0.2382, -1.2085,  1.3595, -0.3696, -1.9900, -0.0190, -0.0650,\n",
       "        -0.9113, -0.5200, -0.6983,  1.9928, -0.0696, -0.0888,  1.2333, -0.1239,\n",
       "         0.4439, -1.6203,  0.5667, -0.5945, -0.2421, -0.5269,  2.4877, -0.8806,\n",
       "        -0.2445,  0.7593, -0.3955,  0.2726,  0.8628,  0.2875, -1.2040, -2.2223,\n",
       "        -1.1184,  1.0380,  1.3793,  1.4562,  1.0795,  1.0112, -0.5845,  0.4778,\n",
       "         0.2163, -2.4681,  1.0116, -0.5934, -0.4720, -0.4162,  0.4561, -1.7864,\n",
       "         0.6403, -1.9020, -0.7081,  0.7733, -0.4436,  1.5291, -0.0608,  0.1165,\n",
       "         0.5760,  0.4930,  0.2778,  1.6181, -2.7590,  0.0591,  0.7998, -0.1854],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_no_pos[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9658e+00,  2.7459e-01, -1.2602e+00,  9.4208e-01, -4.1614e-01,\n",
       "        -2.6972e+00, -1.3175e+00,  5.2243e-01, -6.0261e-02, -2.7417e-01,\n",
       "         2.6612e-02,  3.2121e-02,  8.2448e-01, -1.1622e+00,  3.0622e-02,\n",
       "         1.4931e+00,  1.9149e+00, -1.0901e-01,  1.4340e+00, -7.0940e-02,\n",
       "         4.5717e-01, -1.6009e+00,  1.7435e-01, -2.3522e-01, -4.1109e-01,\n",
       "         1.2431e+00, -1.5995e-01,  1.8360e+00, -9.3732e-01,  4.5272e-01,\n",
       "         6.7320e-01, -2.5089e-02, -1.1962e+00, -1.1338e+00,  1.1345e+00,\n",
       "         7.2322e-01, -7.5964e-01, -2.0787e-01,  1.8721e+00, -8.4484e-01,\n",
       "        -1.4141e+00,  1.1036e+00, -1.0093e+00, -3.3924e-01,  3.2352e-01,\n",
       "        -1.2067e+00,  1.6980e-01, -8.9869e-01,  1.4213e+00,  9.3691e-01,\n",
       "        -9.3960e-01, -1.3784e-01, -1.9907e-01, -7.0311e-01, -6.2364e-01,\n",
       "        -2.2677e-01, -8.4872e-01, -5.4525e-01, -9.6562e-01,  1.4417e-02,\n",
       "        -1.7230e+00, -1.3977e+00, -1.3073e+00, -1.1918e+00, -7.9442e-01,\n",
       "        -1.0835e+00, -5.7867e-01, -2.7028e-03, -7.4924e-01, -3.7165e-02,\n",
       "         1.2999e+00,  6.9678e-01, -5.2875e-01, -8.6574e-01, -1.2449e-01,\n",
       "         4.7065e-01, -2.9725e+00, -1.6712e+00, -1.6714e-01,  5.9600e-01,\n",
       "        -4.8462e-01, -9.7460e-01,  7.2455e-01, -9.4915e-01,  1.0678e+00,\n",
       "         7.7451e-01,  7.5015e-01,  4.1249e+00,  8.8190e-01, -1.3372e+00,\n",
       "        -6.7617e-01,  2.6883e-01, -4.8015e-01,  2.7351e-01, -2.4531e-01,\n",
       "        -1.7519e+00, -1.7944e-01,  3.5351e-01, -3.4921e-01,  5.1621e-01,\n",
       "        -2.5955e-01, -9.3992e-01, -1.9352e-01, -4.5815e-01, -4.2095e-01,\n",
       "        -1.9095e+00, -4.9815e-01, -1.1906e+00,  3.1606e-01,  8.6904e-01,\n",
       "        -1.6757e+00,  5.5092e-01,  7.3796e-02, -8.0426e-02,  1.9894e+00,\n",
       "        -1.0267e+00, -5.6618e-01, -7.2109e-01, -4.0589e-01, -8.9112e-01,\n",
       "        -3.9345e-02, -4.2817e-01,  1.8887e+00,  6.2368e-01, -1.8180e-01,\n",
       "        -1.7245e-01, -1.5865e+00,  1.0438e+00,  1.2375e+00,  2.8875e-01,\n",
       "        -1.4190e+00,  1.7117e+00, -4.0456e-01, -1.6482e+00,  6.2160e-02,\n",
       "        -2.5046e-01, -6.2026e-01, -3.7536e-01, -6.7914e-01,  1.9270e+00,\n",
       "         2.3801e-01,  1.1722e-01,  1.6842e+00, -1.0570e-01,  7.3197e-01,\n",
       "        -1.9538e+00,  7.2100e-01, -5.4216e-01, -2.7326e-01, -8.2991e-01,\n",
       "         2.9959e+00, -1.2156e+00,  1.2420e-02,  4.0262e-01, -4.6825e-01,\n",
       "         6.6448e-01,  1.0563e+00,  1.4302e-01, -1.5119e+00, -2.5315e+00,\n",
       "        -1.5060e+00,  1.3548e+00,  1.3099e+00,  1.7201e+00,  9.9550e-01,\n",
       "         5.9398e-01, -5.4123e-01,  4.3357e-01,  7.0281e-02, -2.2019e+00,\n",
       "         1.2362e+00, -4.8935e-01, -2.6970e-01, -6.4772e-01,  6.1216e-01,\n",
       "        -1.5437e+00,  8.7608e-01, -2.1261e+00, -6.5793e-01,  8.0059e-01,\n",
       "        -5.8389e-01,  1.6052e+00, -8.2139e-02, -1.8997e-02,  4.3856e-01,\n",
       "         5.9836e-01,  4.4820e-01,  1.8412e+00, -2.6627e+00,  1.2035e-02,\n",
       "         7.6844e-01, -1.7530e-01], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred, mask_test \u001b[38;5;241m=\u001b[39m encoder(imgs, masks_enc, pos_drop_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, use_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pred, mask_test = encoder(imgs, masks_enc, pos_drop_ratio=0.2, use_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 256])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 256])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([[[1, 1], [2, 2], [3, 3]], \n",
    "                    [[1, 1], [2, 2], [3, 3]], \n",
    "                    [[1, 1], [2, 2], [3, 3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = torch.tensor([[0, 1], [0, 1], [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [2, 2]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [2, 2]],\n",
       "\n",
       "        [[2, 2],\n",
       "         [3, 3]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_masks(x_1, [mask_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test.unsqueeze(-1).expand(-1, -1, encoder.patch_embed.num_patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pred[mask_test.unsqueeze(-1).expand(-1, -1, encoder.patch_embed.num_patches)].view(batch_size, -1, encoder.patch_embed.num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 256])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262144])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[mask_test.unsqueeze(-1).expand(-1, -1, encoder.patch_embed.num_patches)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "gather(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_no_pos \u001b[38;5;241m=\u001b[39m \u001b[43mapply_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/github/ijepa-pos/src/masks/utils.py:19\u001b[0m, in \u001b[0;36mapply_masks\u001b[0;34m(x, masks)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m masks:\n\u001b[1;32m     18\u001b[0m     mask_keep \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m     all_x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_keep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: gather(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "pred_no_pos = apply_masks(pred, [mask_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 256])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 192])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mask = torch.zeros((3, 4), dtype=torch.bool, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_no_pos = torch.tensor([[0, 1],\n",
    "                            [0, 1],\n",
    "                            [1, 2]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False],\n",
       "        [ True,  True, False, False],\n",
       "        [False,  True,  True, False]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask.scatter_(1, mask_no_pos, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.utils.tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch\n",
    ")\n",
    "from src.masks.utils import apply_masks\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        decoder_embed_dim=256,\n",
    "        decoder_num_heads=2,\n",
    "        decoder_depth=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Encoder settings\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Set up the stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # Set up the encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n",
    "                  attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Patch settings\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size[0],\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Position settings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Mask token settings\n",
    "        self.mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Decoder settings (a light weight decoder just for position prediction)\n",
    "        # Require additional parameters:\n",
    "        # - decoder_emebed_dim\n",
    "        # - decoder_num_heads\n",
    "        # - decoder_depth\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(dim=decoder_embed_dim, num_heads=decoder_num_heads,\n",
    "                  mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, num_patches, bias=True)\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------- #\n",
    "        # Weight Initialiazation\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "        # $$$$ Also initialize the mask_pos_token\n",
    "        torch.nn.init.normal_(self.mask_pos_token, std=.02)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, N, D = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, mask.size(1), device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_pos_drop_mask(self, x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "        B, _, D = x.shape  # Original shape of x\n",
    "        device = x.device\n",
    "\n",
    "        # Determine the number of positions to drop in the masked area\n",
    "        N_m = mask.size(1)  # Number of patches to keep after the mask is applied\n",
    "        num_pos_to_drop = int(N_m * pos_drop_ratio)\n",
    "\n",
    "        # Shuffle mask along the last dimension\n",
    "        random_tensor = torch.rand(B, N_m, device=device)\n",
    "        shuffled_indices = random_tensor.argsort(dim=1)\n",
    "        shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "        # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "        mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "        mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "        # Apply the masks to x\n",
    "        x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "        x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "        # Apply pos_embed and mask_pos_token accordingly\n",
    "        mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1).to(device)\n",
    "        x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "        pos_embed = pos_embed.repeat(B, 1, 1).to(device)\n",
    "        pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "        x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "        # Concatenate the results and shuffle again to restore the original order\n",
    "        x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "        restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "        x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "        # Create a boolean mask in the shuffled order\n",
    "        shuffled_pos_drop_mask = torch.zeros(B, N_m, dtype=torch.bool, device=device)\n",
    "        shuffled_pos_drop_mask[:, :num_pos_to_drop] = True  # Mark the first num_pos_to_drop as True\n",
    "\n",
    "        # Restore the order of the boolean mask to match x_restored\n",
    "        pos_bool = shuffled_pos_drop_mask.gather(1, restored_indices)\n",
    "\n",
    "        # The pos_drop_bool is used to apply on x to get the ones\n",
    "        # whose positional embeddings are dropped\n",
    "        # to apply it, you should you use it like\n",
    "        # x_ = x[pos_drop_bool.unsqueeze(-1).expand(-1, -1, D)].reshape(B, -1, D)\n",
    "        # Differently, mask_no_pos contains the original indices (no. of the patch)\n",
    "        # and will be used as labels\n",
    "\n",
    "        return x, pos_bool, mask_no_pos\n",
    "\n",
    "\n",
    "    def forward_decoder(self, x):\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)  # from decoder_embed_dim to num_patches\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, masks=None, pos_drop_ratio=0, use_decoder=False):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "\n",
    "        # When we do not drop the positional embeddings:\n",
    "        if not pos_drop_ratio:\n",
    "            x += pos_embed\n",
    "\n",
    "            if masks is not None:\n",
    "                x = apply_masks(x, masks)\n",
    "\n",
    "        else:\n",
    "            assert len(masks) == 1, 'Only one mask is needed for the context.'\n",
    "            x, pos_bool, mask_no_pos = self.apply_pos_drop_mask(\n",
    "                x, pos_embed, self.mask_pos_token, masks[0], pos_drop_ratio)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if use_decoder:\n",
    "            assert pos_drop_ratio, 'The function is only tested when pos are dropped.'\n",
    "            logits = self.forward_decoder(x)\n",
    "            return x, logits, pos_bool, mask_no_pos\n",
    "\n",
    "        else:\n",
    "            return x  # The classical IJEPA\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim).to(device)\n",
    "\n",
    "# result = encoder(imgs, masks_enc, pos_drop_ratio=0.2)\n",
    "# x_no_pos, x, pos_drop_mask, x_initial, mask_no_pos = result\n",
    "# kk = x[pos_drop_mask.unsqueeze(-1).expand(-1, -1, 192)].view(batch_size, -1, 192)\n",
    "# dd = apply_masks(x_initial, [mask_no_pos])\n",
    "# sorted_tensor, sorted_indices = torch.sort(mask_no_pos, dim=1)\n",
    "# ee = apply_masks(x_initial, [sorted_tensor])\n",
    "# kk == ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = encoder(imgs, masks_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 192])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = encoder(imgs, masks_enc, pos_drop_ratio=0.4, use_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, logits, pos_bool, labels = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 256])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ...,  True, False,  True],\n",
       "        [False, False,  True,  ...,  True, False,  True],\n",
       "        ...,\n",
       "        [ True,  True, False,  ..., False,  True,  True],\n",
       "        [False, False,  True,  ...,  True, False, False],\n",
       "        [False,  True, False,  ..., False,  True, False]], device='cuda:0')"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[pos_bool.unsqueeze(-1).expand(-1, -1, encoder.patch_embed.num_patches)].reshape(batch_size, -1, encoder.patch_embed.num_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[pos_bool.unsqueeze(-1).expand(-1, -1, encoder.patch_embed.num_patches)].reshape(batch_size, -1, encoder.patch_embed.num_patches)\n",
    "loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 256])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pos = F.cross_entropy(logits.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6013, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, D = result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_predictor = nn.Linear(encoder.embed_dim, \n",
    "                          encoder.patch_embed.num_patches, \n",
    "                          bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pos_predictor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 256])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = F.softmax(pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 256])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange(90).repeat(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    N, L = mask.shape\n",
    "    num_vis = pred.shape[1]\n",
    "    labels = torch.arange(L).repeat(N, 1).to(pred.device).detach()\n",
    "    labels = torch.gather(labels, dim=1, index=ids_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 256])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6813, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(pred.permute(0, 2, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 256])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 77])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 256], got [64, 77]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 256], got [64, 77]"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(r, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           # Calculate the number of positions to drop for the current mask\n",
    "            num_to_drop = int(mask.shape[1] * pos_drop_ratio)\n",
    "            print(num_to_drop)\n",
    "\n",
    "            # Randomly select the indices to be dropped\n",
    "            drop_indices = torch.randperm(mask.shape[1], device=device)[:num_to_drop]\n",
    "            \n",
    "            # Gather the indices from the mask that will be dropped\n",
    "            drop_indices_masked = torch.index_select(mask, 1, drop_indices)\n",
    "            \n",
    "            # Create a mask for positions that are not dropped\n",
    "            non_drop_indices = torch.ones(N, dtype=torch.bool, device=device)\n",
    "            non_drop_indices[drop_indices_masked] = False\n",
    "            \n",
    "            # Add positional embeddings to the non-dropped positions\n",
    "            x[b_idx, non_drop_indices] += pos_embed[b_idx, non_drop_indices]\n",
    "            \n",
    "            # Add mask_pos_token to the dropped positions\n",
    "            x[b_idx, drop_indices_masked] = mask_pos_token.expand_as(x[b_idx, drop_indices_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pos_drop_mask(x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "    B, N, D = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    # Determine the number of positions to drop in the masked area\n",
    "    num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "    # Shuffle mask along the last dimension\n",
    "    random_tensor = torch.rand(B, N_m)\n",
    "    perms = random_tensor.argsort(dim=1)\n",
    "    shuffled_mask = mask.gather(1, perms)\n",
    "\n",
    "    # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "    mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "    mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "    # Apply the masks to x\n",
    "    x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "    x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "\n",
    "    # Apply pos_embed and mask_pos_token accordingly\n",
    "    pos_embed = pos_embed.repeat(B, 1, 1)\n",
    "    pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "    x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "    mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1)\n",
    "    x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "    # Concatenate the results and shuffle again to restore the original order\n",
    "    x_concat = torch.cat([x_keep_pos, x_no_pos], dim=1)\n",
    "\n",
    "\n",
    "    return x_restored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pos_drop_mask(x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "    B, N, D = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    # Determine the number of positions to drop in the masked area\n",
    "    num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "    # Shuffle mask along the last dimension\n",
    "    random_tensor = torch.rand(B, N_m)\n",
    "    shuffled_indices = random_tensor.argsort(dim=1)\n",
    "    shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "    # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "    mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "    mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "    # Apply the masks to x\n",
    "    x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "    x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "    # Apply pos_embed and mask_pos_token accordingly\n",
    "    mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1)\n",
    "    x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "    pos_embed = pos_embed.repeat(B, 1, 1)\n",
    "    pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "    x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "    # Concatenate the results and shuffle again to restore the original order\n",
    "    x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "    restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "    x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoder_.patch_embed(imgs)\n",
    "pos_embed = encoder_.interpolate_pos_encoding(x, encoder_.pos_embed)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "    B, N, D = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    # Determine the number of positions to drop in the masked area\n",
    "    num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "    # Shuffle mask along the last dimension\n",
    "    random_tensor = torch.rand(B, N_m)\n",
    "    shuffled_indices = random_tensor.argsort(dim=1)\n",
    "    shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "    # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "    mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "    mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "    # Apply the masks to x\n",
    "    x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "    x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "    # # Apply pos_embed and mask_pos_token accordingly\n",
    "    # mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1)\n",
    "    # x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "    # pos_embed = pos_embed.repeat(B, 1, 1)\n",
    "    # pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "    # x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "    # Concatenate the results and shuffle again to restore the original order\n",
    "    x_concat = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "    restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "    x_restored = x_concat.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = apply_masks(x, [mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ == x_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0208, -0.7616,  0.4506, -0.1220, -0.9622,  0.4491,  0.5946,  0.2430,\n",
       "        -0.6636,  1.4046,  0.0318, -0.6683, -0.9617,  1.0837, -0.2786,  0.0447,\n",
       "         0.8781,  1.4150,  0.0974,  0.2760, -0.0513, -1.3648, -0.4059,  0.4779,\n",
       "        -0.4468, -1.4697, -1.6139, -0.2443, -1.0343, -0.3135, -0.0034, -0.0478,\n",
       "        -0.7251,  0.5278,  0.0551, -0.2267,  1.5084,  0.0426,  1.6751,  0.5833,\n",
       "        -0.0367, -0.3562,  0.3727,  0.2418,  0.6569,  0.3649,  1.3766, -0.3369,\n",
       "         0.6524,  0.2756, -0.6972,  0.2613,  0.7315, -0.9653, -1.7160,  0.3165,\n",
       "         1.5662,  0.6354,  0.3665, -1.1100,  0.7692,  0.0204,  1.2124,  1.1574,\n",
       "        -0.3337,  0.6425, -0.4855, -0.4052, -0.9536, -0.0757,  0.0626, -0.3468,\n",
       "         0.0157,  0.7922, -0.4741, -1.3195,  0.9651,  0.2866, -0.0285,  1.4460,\n",
       "         0.8951, -0.9021, -0.7752,  0.1132,  0.1739, -1.2930, -0.1893,  0.2564,\n",
       "         0.3589,  0.3718,  0.2388, -0.2554, -0.1476,  0.8896,  0.5945, -0.2822,\n",
       "         0.0288,  0.9150,  0.7384,  0.3057, -2.3194,  0.4594,  0.2083,  0.8882,\n",
       "        -0.1651,  0.5102, -0.3375,  0.8395, -1.2949,  0.1536,  0.2265,  0.0797,\n",
       "        -0.9860, -0.8193, -0.3163, -0.3157, -0.1377, -0.8866,  2.6569, -0.7614,\n",
       "        -0.3692,  0.0643, -1.1831, -0.7077, -0.0372,  1.0398, -0.5319, -0.7611,\n",
       "         0.3816,  0.0662,  0.0899,  1.3535, -0.2308,  0.4933, -0.2034, -0.8969,\n",
       "        -0.4038,  0.1489,  0.3636, -0.3191, -1.2487,  0.6970,  1.6393,  0.6727,\n",
       "        -1.4898,  0.4178, -0.5970, -0.3263, -0.7705, -0.2641,  0.8938, -0.0894,\n",
       "        -0.1385,  0.7640,  0.7161, -0.1495, -0.5680,  0.5305,  1.2426,  0.6325,\n",
       "         0.3445,  0.9980, -0.7657,  0.2895,  1.0799, -0.9957, -0.2402,  0.1269,\n",
       "        -0.1650, -0.7342,  1.9025, -0.1208,  0.1326, -0.2360, -2.3148, -0.3842,\n",
       "         0.0265,  0.0953,  0.4032,  0.4636, -0.7827,  1.0382, -0.7037,  0.5334,\n",
       "        -0.2627,  0.5979, -0.9813,  0.3707, -0.2032, -0.2516,  0.7671,  0.4377],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_restored[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41, 69, 59,  ..., 60, 45, 35],\n",
       "        [15, 21, 51,  ..., 47, 33,  6],\n",
       "        [68,  5, 40,  ..., 66, 48, 44],\n",
       "        ...,\n",
       "        [36, 41, 61,  ..., 16,  1, 71],\n",
       "        [36, 68, 39,  ..., 72, 56, 38],\n",
       "        [47, 19, 76,  ..., 50, 20, 29]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(perms, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15,  9, 59,  ..., 53, 16, 21],\n",
       "        [10, 38, 48,  ...,  3, 31, 71],\n",
       "        [ 9, 39, 70,  ..., 58, 21, 18],\n",
       "        ...,\n",
       "        [ 4, 75, 51,  ..., 10, 56, 32],\n",
       "        [42, 53, 51,  ...,  9, 15, 36],\n",
       "        [51,  9, 21,  ...,  8, 35,  2]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "        [  0,   1,   2,  ..., 198, 199, 200],\n",
       "        [  0,   1,   2,  ..., 146, 147, 148],\n",
       "        ...,\n",
       "        [  6,   7,   8,  ..., 226, 227, 228],\n",
       "        [  0,   1,   2,  ..., 131, 132, 141],\n",
       "        [  0,   1,   2,  ..., 149, 158, 160]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16,   9, 126,  ..., 110,  17,  28],\n",
       "        [ 17,  66,  94,  ...,   3,  52, 195],\n",
       "        [ 16,  67, 133,  ..., 112,  35,  32],\n",
       "        ...,\n",
       "        [ 10, 227, 201,  ...,  23, 206, 142],\n",
       "        [ 76,  94,  92,  ...,  16,  28,  64],\n",
       "        [ 99,  16,  35,  ...,  14,  65,   2]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "    mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_no_pos = x_no_pos + mask_pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_concat = torch.cat([x_keep_pos, x_no_pos], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 192])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15, 192])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_no_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15, 192])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encoder_.patch_embed(imgs)\n",
    "pos_embed = encoder_.interpolate_pos_encoding(x, encoder_.pos_embed)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pos_embed = pos_embed.repeat(B, 1, 1)\n",
    "    pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "    x_no_pos = apply_masks(x, [mask_no_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 62, 192])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_keep_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_keep_pos = x_keep_pos + pos_embed_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 62, 192])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_ = pos_embed.repeat(B, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, D = x.shape\n",
    "num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "B, N = 2, 3\n",
    "tensor = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "\n",
    "# Create random permutations for each row in a batched way\n",
    "perms = torch.randperm(N).repeat(B, 1)\n",
    "print(perms)\n",
    "\n",
    "# Shuffle each row according to its permutation\n",
    "shuffled_tensor = tensor.gather(1, perms)\n",
    "\n",
    "print(shuffled_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N_m = mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "        [  0,   1,   2,  ..., 198, 199, 200],\n",
       "        [  0,   1,   2,  ..., 146, 147, 148],\n",
       "        ...,\n",
       "        [  6,   7,   8,  ..., 226, 227, 228],\n",
       "        [  0,   1,   2,  ..., 131, 132, 141],\n",
       "        [  0,   1,   2,  ..., 149, 158, 160]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.rand(B, N_m)\n",
    "perms = random_tensor.argsort(dim=1)\n",
    "shuffled_mask = mask.gather(1, perms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[162, 142,  80,  ..., 126,  83,  65],\n",
       "        [ 32,  81, 192,  ...,  53,  14,  33],\n",
       "        [ 27,  94, 128,  ...,  12, 142,  99],\n",
       "        ...,\n",
       "        [226,  12, 217,  ...,  71,   7, 218],\n",
       "        [ 36, 141,  92,  ...,  12,  33,  46],\n",
       "        [ 66,  48,   5,  ...,  96,  35,  39]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "B, N = 2, 3\n",
    "tensor = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
    "\n",
    "# Generate a random tensor and use argsort to get unique permutations for each row\n",
    "random_tensor = torch.rand(B, N)\n",
    "perms = random_tensor.argsort(dim=1)\n",
    "\n",
    "# Shuffle each row according to its unique permutation\n",
    "shuffled_tensor = tensor.gather(1, perms)\n",
    "\n",
    "print(shuffled_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1963, 0.0848, 0.1815],\n",
       "        [0.3819, 0.5320, 0.8059]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(B, N)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perms = random_tensor.argsort(dim=1)\n",
    "perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49, 22, 65,  ...,  1, 71, 13],\n",
       "        [49, 22, 65,  ...,  1, 71, 13],\n",
       "        [49, 22, 65,  ...,  1, 71, 13],\n",
       "        ...,\n",
       "        [49, 22, 65,  ...,  1, 71, 13],\n",
       "        [49, 22, 65,  ...,  1, 71, 13],\n",
       "        [49, 22, 65,  ...,  1, 71, 13]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = torch.randperm(mask.size(1), device=device).repeat(B, 1)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    shuffled_indices = torch.randperm(mask.size(1), device=device).repeat(B, 1)\n",
    "    shuffled_mask = torch.gather(mask, 1, shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder_.patch_embed(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = encoder_.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos_tokens = mask_pos_token.repeat(B, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 192])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 192])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 192])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_.pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " int(77* 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, D = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for all positions initially set to keep (1)\n",
    "full_mask = torch.ones((B, N), dtype=torch.bool, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = apply_masks(x, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 192])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the mask for easier processing\n",
    "flat_mask = mask.flatten()  # This mask is of size (B, no. patches kepts for context)\n",
    "\n",
    "# Determine total number of positions to drop across the batch\n",
    "total_num_to_drop = int(flat_mask.numel() * pos_drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a mask for all positions initially set to keep (1)\n",
    "    full_mask = torch.ones((B, N), dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Flatten the mask for easier processing\n",
    "    flat_mask = mask.flatten()\n",
    "\n",
    "    # Determine total number of positions to drop across the batch\n",
    "    total_num_to_drop = int(flat_mask.numel() * pos_drop_ratio)\n",
    "\n",
    "    # Randomly select indices to be dropped from the flattened mask\n",
    "    drop_indices = torch.randperm(flat_mask.numel(), device=device)[:total_num_to_drop]\n",
    "    \n",
    "    # Mark the selected indices for dropping in the full mask\n",
    "    full_mask.view(-1)[flat_mask[drop_indices]] = False\n",
    "\n",
    "    # Apply the positional embeddings for non-dropped positions\n",
    "    x[full_mask] += pos_embed.view(B * N, -1)[full_mask.view(-1)]\n",
    "    \n",
    "    # Apply mask_pos_token for dropped positions\n",
    "    x[~full_mask] = mask_pos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_mask = mask.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "        [  0,   1,   2,  ..., 198, 199, 200],\n",
       "        [  0,   1,   2,  ..., 146, 147, 148],\n",
       "        ...,\n",
       "        [  6,   7,   8,  ..., 226, 227, 228],\n",
       "        [  0,   1,   2,  ..., 131, 132, 141],\n",
       "        [  0,   1,   2,  ..., 149, 158, 160]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4928])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_drop_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4928"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_mask.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_to_drop = int(flat_mask.numel() * pos_drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks_enc[0]\n",
    "pos_drop_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices = torch.randperm(flat_mask.numel(), device=device)[:total_num_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4928"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_mask.numel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mask.view(-1).shape\n",
    "#[flat_mask[drop_indices]] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(mask.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = masks_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "         [  0,   1,   2,  ..., 198, 199, 200],\n",
       "         [  0,   1,   2,  ..., 146, 147, 148],\n",
       "         ...,\n",
       "         [  6,   7,   8,  ..., 226, 227, 228],\n",
       "         [  0,   1,   2,  ..., 131, 132, 141],\n",
       "         [  0,   1,   2,  ..., 149, 158, 160]])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "        [  0,   1,   2,  ..., 198, 199, 200],\n",
       "        [  0,   1,   2,  ..., 146, 147, 148],\n",
       "        ...,\n",
       "        [  6,   7,   8,  ..., 226, 227, 228],\n",
       "        [  0,   1,   2,  ..., 131, 132, 141],\n",
       "        [  0,   1,   2,  ..., 149, 158, 160]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "            num_to_drop = int(mask.shape[1] * pos_drop_ratio)\n",
    "            print(num_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices = torch.randperm(mask.shape[1], device=device)[:num_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 53, 71, 66, 55, 24, 75, 20, 60, 72, 45, 29, 32, 47, 13])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 7, 5, 4, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(10)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward_encoder(self, x, mask_ratio, pos_mask_ratio):\n",
    "        outs = {}\n",
    "        inputs = x.detach().clone()\n",
    "\n",
    "        # embed patches w/o [cls] token\n",
    "        x = self.patch_embed(x)\n",
    "        N, L, D = x.shape\n",
    "\n",
    "        # generate mask\n",
    "        ids_keep, mask, ids_restore, ids_remove = self.random_masking(x, mask_ratio)\n",
    "        outs['mask'], outs['ids_keep'], outs['ids_restore'] = mask, ids_keep, ids_restore\n",
    "        # gather patch embeddings and position embeddings\n",
    "        x = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        pos_embed_all = self.pos_embed[:, 1:, :].data.repeat(N, 1, 1)  # w/o [cls] token\n",
    "        pos_embed_vis = torch.gather(pos_embed_all, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)).detach()\n",
    "\n",
    "        # random masking for position embedding\n",
    "        ids_keep_pos, mask_pos, ids_restore_pos, ids_remove_pos = self.random_masking(x, pos_mask_ratio)\n",
    "        outs['mask_pos'], outs['ids_keep_pos'], outs['ids_restore_pos'] = mask_pos, ids_keep_pos, ids_restore_pos\n",
    "\n",
    "        # gather position embeddings\n",
    "        pos_embed = torch.gather(pos_embed_vis, dim=1, index=ids_keep_pos.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # append mask tokens to position embeddings\n",
    "        mask_pos_length = mask_pos.sum().item()\n",
    "        if self.mask_token_type == 'param':\n",
    "            mask_pos_tokens = self.mask_pos_token.repeat(N, mask_pos_length, 1)\n",
    "        elif self.mask_token_type == 'zeros':\n",
    "            mask_pos_tokens = torch.zeros((N, mask_pos_length, self.embed_dim)).to(x.device)\n",
    "        elif self.mask_token_type == 'wrong_pos':\n",
    "            removed_pos_embed = torch.gather(pos_embed_vis, dim=1, index=ids_remove_pos.unsqueeze(-1).repeat(1, 1, D))\n",
    "            # convert to numpy, since numpy shuffles the first dimension, we have to transpose first\n",
    "            removed_pos_embed = removed_pos_embed.detach().cpu().permute(1, 0, 2).numpy()        # [N, L, D] -> [L, N, D]\n",
    "            np.random.shuffle(removed_pos_embed)\n",
    "            # restore to torch\n",
    "            removed_pos_embed = torch.from_numpy(removed_pos_embed).permute(1, 0, 2)    # [L, N, D] -> [N, L, D]\n",
    "            mask_pos_tokens = removed_pos_embed.to(x.device)\n",
    "        else:\n",
    "            raise Exception('unknown mask_token_type: {}'.format(self.mask_token_type))\n",
    "\n",
    "        pos_embed = torch.cat([pos_embed, mask_pos_tokens], dim=1)\n",
    "\n",
    "        # restore position embeddings before adding\n",
    "        pos_embed = torch.gather(pos_embed, dim=1, index=ids_restore_pos.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # add position embedding w/o [cls] token\n",
    "        x = x + pos_embed\n",
    "\n",
    "        if self.shuffle:\n",
    "            # generate shuffle indexes first\n",
    "            ids_keep_shuffle, _, ids_restore_shuffle, _ = self.random_masking(x, 0.)\n",
    "\n",
    "            # gather\n",
    "            x = torch.gather(x, dim=1, index=ids_keep_shuffle.unsqueeze(-1).repeat(1, 1, D))\n",
    "            outs['ids_restore_shuffle'] = ids_restore_shuffle\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # get last self-attention\n",
    "        if self.attn_guide:\n",
    "            # get attentions\n",
    "            # attn = self.get_last_attention(inputs)\n",
    "            # attn = attn[:, :, 0, 1:].mean(1)    # [N, num_patches]\n",
    "            # outs['attn_full'] = attn\n",
    "\n",
    "            # get similarities\n",
    "            attn = self.get_feature_similarity(inputs)\n",
    "            outs['attn_full'] = attn\n",
    "\n",
    "            # gather visible patches\n",
    "            attn = torch.gather(attn, dim=1, index=ids_keep)\n",
    "            outs['attn'] = attn / attn.sum(-1, keepdims=True)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        outs['x'] = self.norm(x)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 164, 165, 166],\n",
       "        [  0,   1,   2,  ..., 198, 199, 200],\n",
       "        [  0,   1,   2,  ..., 146, 147, 148],\n",
       "        ...,\n",
       "        [  6,   7,   8,  ..., 226, 227, 228],\n",
       "        [  0,   1,   2,  ..., 131, 132, 141],\n",
       "        [  0,   1,   2,  ..., 149, 158, 160]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder, predictor = init_model(\n",
    "#     device=device,\n",
    "#     patch_size=patch_size,\n",
    "#     crop_size=crop_size,\n",
    "#     pred_depth=pred_depth,\n",
    "#     pred_emb_dim=pred_emb_dim,\n",
    "#     model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m encoder_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_' is not defined"
     ]
    }
   ],
   "source": [
    "del encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = vit_tiny(patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = encoder_.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([64, 256, 192])\n",
      "pos_embed shape torch.Size([1, 256, 192])\n"
     ]
    }
   ],
   "source": [
    "d_ = encoder_(imgs, masks_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 77, 192])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1093632, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d == d_).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "         29,  30,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n",
       "         44,  45,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "         59,  60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
       "         74,  75,  76,  77,  78,  80,  81,  82,  83,  84,  85,  86,  87,  88,\n",
       "         89,  90,  91,  92,  93,  94,  96, 109, 110, 112, 125, 126],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  16,  17,  18,  19,  20,  21,  22,  23,  32,  33,  34,  35,  36,\n",
       "         37,  38,  39,  48,  49,  50,  51,  52,  53,  64,  65,  66,  67,  68,\n",
       "         69,  80,  81,  82,  83,  84,  85,  96,  97,  98,  99, 100, 101, 109,\n",
       "        110, 112, 113, 114, 115, 116, 117, 125, 126, 128, 129, 130, 131, 132,\n",
       "        133, 141, 142, 144, 145, 146, 147, 148, 149, 157, 158, 160, 161, 162,\n",
       "        173, 174, 176, 177, 178, 189, 190, 192, 193, 194, 202, 203],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_m == mask.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pos_drop_mask(x, pos_embed, mask_pos_token, mask, pos_drop_ratio):\n",
    "    B, N, D = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    # Determine the number of positions to drop in the masked area\n",
    "    num_pos_to_drop = int(mask.size(1) * pos_drop_ratio)\n",
    "\n",
    "    # Shuffle mask along the last dimension\n",
    "    random_tensor = torch.rand(B, N_m)\n",
    "    shuffled_indices = random_tensor.argsort(dim=1)\n",
    "    shuffled_mask = mask.gather(1, shuffled_indices)\n",
    "\n",
    "    # Split the mask into two: one for keeping pos_embed, one for mask_pos_token\n",
    "    mask_no_pos = shuffled_mask[:, :num_pos_to_drop]\n",
    "    mask_keep_pos = shuffled_mask[:, num_pos_to_drop:]\n",
    "\n",
    "    # Apply the masks to x\n",
    "    x_no_pos = apply_masks(x, [mask_no_pos])\n",
    "    x_keep_pos = apply_masks(x, [mask_keep_pos])\n",
    "\n",
    "    # Apply pos_embed and mask_pos_token accordingly\n",
    "    mask_pos_tokens = mask_pos_token.repeat(B, num_pos_to_drop, 1)\n",
    "    x_no_pos = x_no_pos + mask_pos_tokens\n",
    "\n",
    "    pos_embed = pos_embed.repeat(B, 1, 1)\n",
    "    pos_embed_masked = apply_masks(pos_embed, [mask_keep_pos])\n",
    "    x_keep_pos = x_keep_pos + pos_embed_masked\n",
    "\n",
    "    # Concatenate the results and shuffle again to restore the original order\n",
    "    x = torch.cat([x_no_pos, x_keep_pos], dim=1)\n",
    "    restored_indices = torch.argsort(shuffled_indices, dim=1)\n",
    "    x = x.gather(1, restored_indices.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 192])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = vit_tiny().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [32,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [33,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [34,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [35,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [36,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [37,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [38,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [39,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [40,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [41,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [42,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [43,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [44,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [45,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [46,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [47,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [48,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [49,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [50,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [51,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [52,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [53,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [54,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [55,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [56,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [57,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [58,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [59,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [60,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [61,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [62,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [63,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [96,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [97,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [98,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [99,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [100,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [101,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [102,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [103,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [104,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [105,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [106,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [107,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [108,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [109,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [110,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [111,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [112,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [113,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [114,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [115,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [116,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [117,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [118,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [119,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [120,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [121,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [122,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [123,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [124,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [125,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [126,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [127,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [2,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [3,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [4,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [5,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [6,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [7,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [8,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [9,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [10,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [11,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [12,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [13,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [14,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [15,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [16,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [17,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [18,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [19,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [20,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [21,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [22,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [23,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [24,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [25,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [26,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [27,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [28,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [29,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [30,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [31,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [64,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [65,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [66,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [67,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [68,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [69,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [70,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [71,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [72,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [73,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [74,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [75,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [76,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [77,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [78,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [79,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [80,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [81,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [82,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [83,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [84,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [85,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [86,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [87,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [88,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [94,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:144: operator(): block: [632,0,0], thread: [95,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dd \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_enc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 305\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# -- fwd prop\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 305\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 159\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 159\u001b[0m     y, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "dd = encoder_(imgs, masks_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 63])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_enc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 196, 192])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
